---
title: 'Lab 2: Avocado Prices'
jupyter: python3
format: 
    html:
        embed-resources: true
---



## Data Set-up

### 0. Import the Dataset

```{python}
import pandas as pd
import numpy as np

df= pd.read_csv("avocado-updated-2020.csv")
```

```{python}
df.head()
```

### 1. Briefly describe the dataset. What information does it contain?

The Avocado dataset contains weekly avocado sales and pricing data from 2015 and on (across different US regions). Each row represents the sales information for a specific date, avocado type, and geographical market. This dataset allows analysts to study avocado sales trends over time.

Key columns:
- date: week that avocados were observed
- average_price: reflects a per unit (per avocado) cost even when multiple units are sold in bags
- total_volume: total # of avocados (units) sold
- 4046: small/medium hass (3-5 oz)
- 4225: large hass (8-0oz)
- 4770: extra large hass avocao (10-15 oz)
- total_bags: # of avocados sold in bags
- small_bags, large_bags, xlarge_bags: size of bags
- type: conventional or organic
- year: year observed
- geography: city where sales recorded

### 2. Clean the data in any way you see fit?

```{python}
#rename lookup column to reflect size
df_clean = df.rename(columns={
    "date": "Date",
    "average_price": "Avg Price",
    "total_volume": "Total Vol",
    "4046": "Small (4046)",
    "4225": "Large (4225)",
    "4770": "XLarge (4770)",
    "total_bags": "Total Bags",
    "small_bags": "Small Bags",
    "large_bags": "Large Bags",
    "xlarge_bags": "XLarge Bags",
    "type": "Type",
    "year": "Year",
    "geography": "Geography"})

#drops any potential rows with missing values and removes duplicates
df_clean = df_clean.dropna()
df_clean = df_clean.drop_duplicates()

#adds new column for total sales of small/medium, large, and xl hass
df_clean["Total Sales"] = df_clean["Small (4046)"] + df_clean["Large (4225)"] + df_clean["XLarge (4770)"]

#changes small, large, xl, and total sales of avocados to integer (whole number)
int_cols = ["Small (4046)", "Large (4225)", "XLarge (4770)",
            "Total Bags", "Small Bags", "Large Bags", "XLarge Bags"]
for col in int_cols:
    df_clean[col] = np.ceil(df_clean[col]).astype("int64")


df_clean.head()
```

```{python}
#unique values in Geography column
df_clean["Geography"].unique()
```

```{python}
# four lists (10, 2, 10, 32, 1)
major_regions = [
    "West", "Northeast", "South Central", "Southeast",
    "Midsouth", "Plains", "Great Lakes",
    "Northern New England", "West Tex/New Mexico"]

states = ["California", "South Carolina"]

metro_areas = [
    "Baltimore/Washington", "Buffalo/Rochester", "Cincinnati/Dayton",
    "Dallas/Ft. Worth", "Hartford/Springfield", "Miami/Ft. Lauderdale",
    "New Orleans/Mobile", "Phoenix/Tucson", "Raleigh/Greensboro",
    "Richmond/Norfolk"]

cities = [
    "Albany", "Atlanta", "Boise", "Boston", "Charlotte", "Chicago",
    "Columbus", "Denver", "Detroit", "Grand Rapids", "Harrisburg/Scranton",
    "Houston", "Indianapolis", "Jacksonville", "Las Vegas", "Los Angeles",
    "Louisville", "Nashville", "New York", "Orlando", "Philadelphia",
    "Pittsburgh", "Portland", "Roanoke", "Sacramento", "San Diego",
    "San Francisco", "Seattle", "Spokane", "St. Louis", "Syracuse", "Tampa"]

national = ["Total U.S."]

# 3) type map (used AI for help)
type_map = {}
type_map.update({g: "major_region" for g in major_regions})
type_map.update({g: "state"         for g in states})
type_map.update({g: "metro_area"    for g in metro_areas})
type_map.update({g: "city"          for g in cities})
type_map.update({g: "national"      for g in national})

# 4) region_type column
df_clean["region_type"] = df_clean["Geography"].map(type_map).fillna("other")

df_clean["major_geographical_region"] = df_clean["Geography"].where(df_clean["Geography"].isin(major_regions))
df_clean["state"]        = df_clean["Geography"].where(df_clean["Geography"].isin(states))
df_clean["metro_area"]   = df_clean["Geography"].where(df_clean["Geography"].isin(metro_areas))
df_clean["city"]         = df_clean["Geography"].where(df_clean["Geography"].isin(cities))
df_clean["national_lvl"] = df_clean["Geography"].where(df_clean["Geography"].isin(national))

df_clean
```

## Exercises

### 3. Which major geographical region sold the most total organic, small Hass avocados in 2017?

```{python}
# filter
problem3 = df_clean.loc[
    (df_clean["Year"] == 2017)
    & (df_clean["Type"].str.lower() == "organic")
    & (df_clean["major_geographical_region"].notna())
    & (df_clean["Small (4046)"] > 0),
    ["Year", "Type", "major_geographical_region", "Small (4046)"]
]

# group by region and sum
problem3 = (
    problem3
    .groupby("major_geographical_region", as_index=False)["Small (4046)"]
    .sum()
    .sort_values(by="Small (4046)", ascending=False)
)
problem3
```

#### Problem 3 Answer: West

### 4. Split the date variable into month, day, and year variables. In which month is the highest average volume of avocado sales?

```{python}
#date column to datetime
df_clean["Date"] = pd.to_datetime(df_clean["Date"])

#month, day, and year into new columns
df_clean["month"] = df_clean["Date"].dt.month
df_clean["day"]   = df_clean["Date"].dt.day
df_clean["year"]  = df_clean["Date"].dt.year

#group by month and compute average total volume
monthly_avg = (
    df_clean.groupby("month")["Total Sales"]
    .mean()
    .sort_values(ascending=False)
)

monthly_avg.head()
```

#### Problem 4 Answer: May

### 5. Which metro area geographical regions sold the most total avocados? Plot side-by-side box-plots of the total volume for only the five metro geographical regions with the highest averages for the total_volume variable.

```{python}
from plotnine import ggplot, aes, geom_boxplot, labs, theme, element_text, geom_bar, facet_wrap, scale_y_continuous, geom_point, geom_text

#filter metro area
metro_df = df_clean.loc[df_clean["region_type"] == "metro_area"]

#find top 5 metro areas by average total_volume
top5_metros = (
    metro_df.groupby("Geography")["Total Vol"]
    .mean()
    .sort_values(ascending=False)
    .head(5)
    .index
)

# only top five metros
top5_df = metro_df.loc[metro_df["Geography"].isin(top5_metros)]

#boxplot
( ggplot(top5_df, aes(x="Geography", y="Total Sales"))
    + geom_boxplot(fill="#9fd8cb", color="black")
    + labs(
        title="Avocado Volume for Highest 5 Metro Areas",
        x="Metro Area",
        y="Total Volume"
    )
    + theme(
        axis_text_x=element_text(size=9),
        figure_size=(9, 5)
    )
)
```

#### Problem 5 Answer: Dallas/Ft. Worth

## Pivoting

### 6. From your cleaned data set, create a data set with only these California regions and answer the following questions about these California regions only.

```{python}
# ca regions
california_regions = ["Los Angeles", "San Francisco", "San Diego", "Sacramento"]

# filter dataset
df_california = df_clean.loc[df_clean["city"].isin(california_regions)].copy()

df_california
```

### 7. In which California regions is the price of organic versus conventional avocados most different? Support your answer with a few summary statistics AND a visualization.

```{python}
#region, type, avg price
avg_price_by_type = (
    df_california.groupby(["Geography", "Type"])["Avg Price"]
    .mean()
    .unstack() #seperate columns, used chat for help
    .reset_index()
)

#new column for the difference
avg_price_by_type["price_diff"] = (
    avg_price_by_type["organic"] - avg_price_by_type["conventional"]
)

#sort difference largest to smallest
avg_price_by_type = avg_price_by_type.sort_values(by="price_diff", ascending=False)

avg_price_by_type
```

```{python}
#barchart
(
ggplot(avg_price_by_type, aes(x="Geography", y="price_diff"))
+ geom_bar(stat="identity", fill="#9fd8cb", color="black")
+ labs(
    title="$ Difference Between Organic and Conventional Avocados (California Regions)",
    x="California Region",
    y="Price Difference"
)
+ theme(
    axis_text_x=element_text(size=9),
    figure_size=(9, 5)
)
)
```

#### Problem 7 Answer: San Francisco

### 8. The following plot shows, for all four California regions, the proportion of the average Hass avocado sales that are small, large, or extra large; conventional vs. organic. Recreate the plot; you do not have to replicate the exact finishing touches - e.g., color, theme - but your plot should resemble the content of this plot.

```{python}
# avg props by region, type, and size
avg_sales = (
    df_california
    .groupby(["Geography", "Type"])
    [["Small (4046)", "Large (4225)", "XLarge (4770)"]]
    .mean()
    .reset_index()
)

# long format reshape (help from AI)
avg_sales_long = avg_sales.melt(
    id_vars=["Geography", "Type"],
    var_name="size",
    value_name="avg_sales"
)

# propd within each region and type (help for AI)
avg_sales_long["proportion"] = (
    avg_sales_long.groupby(["Geography", "Type"])["avg_sales"]
    .apply(lambda x: x / x.sum())
    .reset_index(drop=True)
)

# stacked bar plot
avg_sales_long["proportion"] = avg_sales_long["proportion"] * 100
(
ggplot(avg_sales_long, aes(x="Geography", y="proportion", fill="size"))
+ geom_bar(stat="identity")
+ facet_wrap("~Type")
+ labs(
    title="Proportion of Average Hass Avocado Sales by Size",
    x="Region of California",
    y="Proportion"
)
+ scale_y_continuous(breaks=[0, 25, 50, 75, 100], limits=[0,100])
+ theme(
    axis_text_x=element_text(size=9),
    figure_size=(9, 5)
)
)
```

## Using Outside Data

```{python}
df_housing= pd.read_csv("lab2housing.csv")
```

```{python}
#filter four cities
df_california_homes = df_housing[df_housing["RegionName"].isin(california_regions)]

df_california_homes = df_california_homes[["RegionName", "2019-12"]]
```

```{python}
# filter four cities and avg avo price
df_avocado_city = (
    df_california[df_california["city"].isin(california_regions)]
    .groupby("city")[["Avg Price"]]
    .mean()
    .reset_index() #plz remember this olivia bc city needs to be column not index
)
```

```{python}
# rename columns to match
df_california_homes = df_california_homes.rename(
    columns={"RegionName": "city", "2019-12": "avg_home_price"}
)

# merge avocado and housing datasets
df_please_work = pd.merge(
    df_avocado_city,
    df_california_homes,
    on="city"
)

df_please_work
```

```{python}
(
ggplot(df_please_work, aes(x="avg_home_price", y="Avg Price", label="city"))
+ geom_point(size=3, color="#9fd8cb")
+ geom_text()
+ labs(
    title="Relationship Between Home and Avocado Prices by City",
    x="Average Home price (thousands)",
    y="Average Avocado price ($)")
)
```

#### In our four California regions, we see no association between average housing prices and avocado prices. The housing dataset was created using median housing prices from Zillow. In essence, millennials are probably not living in San Francisco where both the housing and avocado prices are unfortunate. However, since there is no association, we cannot be clear as to the relationship between avocados and housing prices.

